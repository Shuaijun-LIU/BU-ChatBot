{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/21 [00:02<?, ?it/s]\u001B[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 57\u001B[0m\n\u001B[1;32m     55\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m     56\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 57\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m loop\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     60\u001B[0m loop\u001B[38;5;241m.\u001B[39mset_postfix(loss\u001B[38;5;241m=\u001B[39mloss\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py:385\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    381\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    382\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    383\u001B[0m             )\n\u001B[0;32m--> 385\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    388\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adamw.py:187\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    174\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    176\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    177\u001B[0m         group,\n\u001B[1;32m    178\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    184\u001B[0m         state_steps,\n\u001B[1;32m    185\u001B[0m     )\n\u001B[0;32m--> 187\u001B[0m     \u001B[43madamw\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adamw.py:339\u001B[0m, in \u001B[0;36madamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    336\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    337\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adamw\n\u001B[0;32m--> 339\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adamw.py:470\u001B[0m, in \u001B[0;36m_single_tensor_adamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001B[0m\n\u001B[1;32m    468\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    469\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 470\u001B[0m         denom \u001B[38;5;241m=\u001B[39m (\u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m/\u001B[39m bias_correction2_sqrt)\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[1;32m    472\u001B[0m     param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n\u001B[1;32m    474\u001B[0m \u001B[38;5;66;03m# Lastly, switch back to complex view\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "''' NO DETAILS'''\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm  # for displaying progress bar\n",
    "\n",
    "# Load the FAQ data from the JSON file\n",
    "with open('BU_MET_FAQs_small.json', 'r') as file:\n",
    "    faq_data = json.load(file)\n",
    "\n",
    "# Extracting only 'module', 'question', and 'answer' fields from faq_data\n",
    "questions = [{'question': data['question'], 'module': data['module'], 'answer': data['answer']} for data in faq_data]\n",
    "\n",
    "class FAQDataset(Dataset):\n",
    "    def __init__(self, tokenizer, questions):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "\n",
    "        for item in questions:\n",
    "            prompt = f\"{item['question']} [Module: {item['module']}]\"\n",
    "            self.examples.append(tokenizer(prompt, item['answer'], truncation=True, padding='max_length', max_length=512))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {key: torch.tensor(val) for key, val in self.examples[i].items()}\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Prepare the data for the GPT model\n",
    "dataset = FAQDataset(tokenizer, questions)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.train()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['input_ids'])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./trained_gpt_faq_model')\n",
    "\n",
    "# Function to generate answers\n",
    "# def ask_question(question, module):\n",
    "#     model.eval()\n",
    "#     prompt = f\"{question} [Module: {module}]\"\n",
    "#     inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "#     outputs = model.generate(inputs, max_length=200, num_beams=5, temperature=0.7, top_k=50)\n",
    "#     answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "#     return answer\n",
    "\n",
    "def ask_question(question, module):\n",
    "    model.eval()\n",
    "    prompt = f\"{question} [Module: {module}]\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=200)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T01:02:25.958449Z",
     "start_time": "2024-06-12T01:02:22.607766Z"
    }
   },
   "id": "95ef02d9a7c269d5"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the difference between the Master's Degree programs? [Module: Programs]\n",
      "\n",
      "The Master's Degree program is a program that is designed to prepare students for the careers of the next generation of leaders in the field of leadership. The Master's Degree program is designed to prepare students for the careers of the next generation of leaders in the field of leadership.\n",
      "\n",
      "The Master's Degree program is a program that is designed to prepare students for the careers of the next generation of leaders in the field of leadership.\n",
      "\n",
      "The Master's Degree program is a program that is designed to prepare students for the careers of the next generation of leaders in the field of leadership.\n",
      "\n",
      "The Master's Degree program is a program that is designed to prepare students for the careers of the next generation of leaders in the field of leadership.\n",
      "\n",
      "The Master's Degree program is a program that is designed to prepare students for the careers of the next generation of leaders in the field of leadership.\n",
      "\n",
      "The Master\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "print(ask_question(\"What is the difference between the Master's Degree programs?\", \"Programs\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T01:02:32.538370Z",
     "start_time": "2024-06-12T01:02:28.754832Z"
    }
   },
   "id": "7e95370b2292aed1"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 0:   0%|          | 0/21 [00:02<?, ?it/s]\u001B[A\n",
      "Epoch 0:   0%|          | 0/21 [00:02<?, ?it/s, loss=5.94]\u001B[A\n",
      "Epoch 0:   5%|▍         | 1/21 [00:02<00:45,  2.30s/it, loss=5.94]\u001B[A\n",
      "Epoch 0:   5%|▍         | 1/21 [00:04<00:45,  2.30s/it, loss=5.94]\u001B[A\n",
      "Epoch 0:   5%|▍         | 1/21 [00:04<00:45,  2.30s/it, loss=3.39]\u001B[A\n",
      "Epoch 0:  10%|▉         | 2/21 [00:04<00:38,  2.01s/it, loss=3.39]\u001B[A\n",
      "Epoch 0:  10%|▉         | 2/21 [00:05<00:38,  2.01s/it, loss=3.39]\u001B[A\n",
      "Epoch 0:  10%|▉         | 2/21 [00:05<00:38,  2.01s/it, loss=4.64]\u001B[A\n",
      "Epoch 0:  14%|█▍        | 3/21 [00:05<00:34,  1.93s/it, loss=4.64]\u001B[A\n",
      "Epoch 0:  14%|█▍        | 3/21 [00:07<00:34,  1.93s/it, loss=4.64]\u001B[A\n",
      "Epoch 0:  14%|█▍        | 3/21 [00:07<00:34,  1.93s/it, loss=2.65]\u001B[A\n",
      "Epoch 0:  19%|█▉        | 4/21 [00:07<00:31,  1.84s/it, loss=2.65]\u001B[A\n",
      "Epoch 0:  19%|█▉        | 4/21 [00:09<00:31,  1.84s/it, loss=2.65]\u001B[A\n",
      "Epoch 0:  19%|█▉        | 4/21 [00:09<00:31,  1.84s/it, loss=2.51]\u001B[A\n",
      "Epoch 0:  24%|██▍       | 5/21 [00:09<00:28,  1.80s/it, loss=2.51]\u001B[A\n",
      "Epoch 0:  24%|██▍       | 5/21 [00:11<00:28,  1.80s/it, loss=2.51]\u001B[A\n",
      "Epoch 0:  24%|██▍       | 5/21 [00:11<00:28,  1.80s/it, loss=2.12]\u001B[A\n",
      "Epoch 0:  29%|██▊       | 6/21 [00:11<00:26,  1.75s/it, loss=2.12]\u001B[A\n",
      "Epoch 0:  29%|██▊       | 6/21 [00:12<00:26,  1.75s/it, loss=2.12]\u001B[A\n",
      "Epoch 0:  29%|██▊       | 6/21 [00:12<00:26,  1.75s/it, loss=1.23]\u001B[A\n",
      "Epoch 0:  33%|███▎      | 7/21 [00:12<00:24,  1.72s/it, loss=1.23]\u001B[A\n",
      "Epoch 0:  33%|███▎      | 7/21 [00:14<00:24,  1.72s/it, loss=1.23]\u001B[A\n",
      "Epoch 0:  33%|███▎      | 7/21 [00:14<00:24,  1.72s/it, loss=4.12]\u001B[A\n",
      "Epoch 0:  38%|███▊      | 8/21 [00:14<00:22,  1.72s/it, loss=4.12]\u001B[A\n",
      "Epoch 0:  38%|███▊      | 8/21 [00:16<00:22,  1.72s/it, loss=4.12]\u001B[A\n",
      "Epoch 0:  38%|███▊      | 8/21 [00:16<00:22,  1.72s/it, loss=0.672]\u001B[A\n",
      "Epoch 0:  43%|████▎     | 9/21 [00:16<00:20,  1.72s/it, loss=0.672]\u001B[A\n",
      "Epoch 0:  43%|████▎     | 9/21 [00:17<00:20,  1.72s/it, loss=0.672]\u001B[A\n",
      "Epoch 0:  43%|████▎     | 9/21 [00:17<00:20,  1.72s/it, loss=2.72] \u001B[A\n",
      "Epoch 0:  48%|████▊     | 10/21 [00:17<00:18,  1.70s/it, loss=2.72]\u001B[A\n",
      "Epoch 0:  48%|████▊     | 10/21 [00:19<00:18,  1.70s/it, loss=2.72]\u001B[A\n",
      "Epoch 0:  48%|████▊     | 10/21 [00:19<00:18,  1.70s/it, loss=2.09]\u001B[A\n",
      "Epoch 0:  52%|█████▏    | 11/21 [00:19<00:16,  1.68s/it, loss=2.09]\u001B[A\n",
      "Epoch 0:  52%|█████▏    | 11/21 [00:21<00:16,  1.68s/it, loss=2.09]\u001B[A\n",
      "Epoch 0:  52%|█████▏    | 11/21 [00:21<00:16,  1.68s/it, loss=2.11]\u001B[A\n",
      "Epoch 0:  57%|█████▋    | 12/21 [00:21<00:14,  1.67s/it, loss=2.11]\u001B[A\n",
      "Epoch 0:  57%|█████▋    | 12/21 [00:22<00:14,  1.67s/it, loss=2.11]\u001B[A\n",
      "Epoch 0:  57%|█████▋    | 12/21 [00:22<00:14,  1.67s/it, loss=2.33]\u001B[A\n",
      "Epoch 0:  62%|██████▏   | 13/21 [00:22<00:13,  1.68s/it, loss=2.33]\u001B[A\n",
      "Epoch 0:  62%|██████▏   | 13/21 [00:24<00:13,  1.68s/it, loss=2.33]\u001B[A\n",
      "Epoch 0:  62%|██████▏   | 13/21 [00:24<00:13,  1.68s/it, loss=0.856]\u001B[A\n",
      "Epoch 0:  67%|██████▋   | 14/21 [00:24<00:11,  1.68s/it, loss=0.856]\u001B[A\n",
      "Epoch 0:  67%|██████▋   | 14/21 [00:26<00:11,  1.68s/it, loss=0.856]\u001B[A\n",
      "Epoch 0:  67%|██████▋   | 14/21 [00:26<00:11,  1.68s/it, loss=1.77] \u001B[A\n",
      "Epoch 0:  71%|███████▏  | 15/21 [00:26<00:10,  1.67s/it, loss=1.77]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 15/21 [00:27<00:10,  1.67s/it, loss=1.77]\u001B[A\n",
      "Epoch 0:  71%|███████▏  | 15/21 [00:27<00:10,  1.67s/it, loss=1.92]\u001B[A\n",
      "Epoch 0:  76%|███████▌  | 16/21 [00:27<00:08,  1.70s/it, loss=1.92]\u001B[A\n",
      "Epoch 0:  76%|███████▌  | 16/21 [00:29<00:08,  1.70s/it, loss=1.92]\u001B[A\n",
      "Epoch 0:  76%|███████▌  | 16/21 [00:29<00:08,  1.70s/it, loss=2.03]\u001B[A\n",
      "Epoch 0:  81%|████████  | 17/21 [00:29<00:06,  1.70s/it, loss=2.03]\u001B[A\n",
      "Epoch 0:  81%|████████  | 17/21 [00:31<00:06,  1.70s/it, loss=2.03]\u001B[A\n",
      "Epoch 0:  81%|████████  | 17/21 [00:31<00:06,  1.70s/it, loss=0.564]\u001B[A\n",
      "Epoch 0:  86%|████████▌ | 18/21 [00:31<00:05,  1.71s/it, loss=0.564]\u001B[A\n",
      "Epoch 0:  86%|████████▌ | 18/21 [00:32<00:05,  1.71s/it, loss=0.564]\u001B[A\n",
      "Epoch 0:  86%|████████▌ | 18/21 [00:32<00:05,  1.71s/it, loss=1.7]  \u001B[A\n",
      "Epoch 0:  90%|█████████ | 19/21 [00:32<00:03,  1.70s/it, loss=1.7]\u001B[A\n",
      "Epoch 0:  90%|█████████ | 19/21 [00:34<00:03,  1.70s/it, loss=1.7]\u001B[A\n",
      "Epoch 0:  90%|█████████ | 19/21 [00:34<00:03,  1.70s/it, loss=1.85]\u001B[A\n",
      "Epoch 0:  95%|█████████▌| 20/21 [00:34<00:01,  1.70s/it, loss=1.85]\u001B[A\n",
      "Epoch 0:  95%|█████████▌| 20/21 [00:36<00:01,  1.70s/it, loss=1.85]\u001B[A\n",
      "Epoch 0:  95%|█████████▌| 20/21 [00:36<00:01,  1.70s/it, loss=0.826]\u001B[A\n",
      "Epoch 0: 100%|██████████| 21/21 [00:36<00:00,  1.73s/it, loss=0.826]\u001B[A\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 1:   0%|          | 0/21 [00:01<?, ?it/s]\u001B[A\n",
      "Epoch 1:   0%|          | 0/21 [00:01<?, ?it/s, loss=3.19]\u001B[A\n",
      "Epoch 1:   5%|▍         | 1/21 [00:01<00:32,  1.62s/it, loss=3.19]\u001B[A\n",
      "Epoch 1:   5%|▍         | 1/21 [00:03<00:32,  1.62s/it, loss=3.19]\u001B[A\n",
      "Epoch 1:   5%|▍         | 1/21 [00:03<00:32,  1.62s/it, loss=1.85]\u001B[A\n",
      "Epoch 1:  10%|▉         | 2/21 [00:03<00:31,  1.66s/it, loss=1.85]\u001B[A\n",
      "Epoch 1:  10%|▉         | 2/21 [00:04<00:31,  1.66s/it, loss=1.85]\u001B[A\n",
      "Epoch 1:  10%|▉         | 2/21 [00:04<00:31,  1.66s/it, loss=0.631]\u001B[A\n",
      "Epoch 1:  14%|█▍        | 3/21 [00:04<00:30,  1.67s/it, loss=0.631]\u001B[A\n",
      "Epoch 1:  14%|█▍        | 3/21 [00:06<00:30,  1.67s/it, loss=0.631]\u001B[A\n",
      "Epoch 1:  14%|█▍        | 3/21 [00:06<00:30,  1.67s/it, loss=0.903]\u001B[A\n",
      "Epoch 1:  19%|█▉        | 4/21 [00:06<00:28,  1.69s/it, loss=0.903]\u001B[A\n",
      "Epoch 1:  19%|█▉        | 4/21 [00:08<00:28,  1.69s/it, loss=0.903]\u001B[A\n",
      "Epoch 1:  19%|█▉        | 4/21 [00:08<00:28,  1.69s/it, loss=2.9]  \u001B[A\n",
      "Epoch 1:  24%|██▍       | 5/21 [00:08<00:27,  1.71s/it, loss=2.9]\u001B[A\n",
      "Epoch 1:  24%|██▍       | 5/21 [00:10<00:27,  1.71s/it, loss=2.9]\u001B[A\n",
      "Epoch 1:  24%|██▍       | 5/21 [00:10<00:27,  1.71s/it, loss=1.79]\u001B[A\n",
      "Epoch 1:  29%|██▊       | 6/21 [00:10<00:25,  1.69s/it, loss=1.79]\u001B[A\n",
      "Epoch 1:  29%|██▊       | 6/21 [00:11<00:25,  1.69s/it, loss=1.79]\u001B[A\n",
      "Epoch 1:  29%|██▊       | 6/21 [00:11<00:25,  1.69s/it, loss=1.75]\u001B[A\n",
      "Epoch 1:  33%|███▎      | 7/21 [00:11<00:23,  1.68s/it, loss=1.75]\u001B[A\n",
      "Epoch 1:  33%|███▎      | 7/21 [00:13<00:23,  1.68s/it, loss=1.75]\u001B[A\n",
      "Epoch 1:  33%|███▎      | 7/21 [00:13<00:23,  1.68s/it, loss=1.96]\u001B[A\n",
      "Epoch 1:  38%|███▊      | 8/21 [00:13<00:21,  1.67s/it, loss=1.96]\u001B[A\n",
      "Epoch 1:  38%|███▊      | 8/21 [00:15<00:21,  1.67s/it, loss=1.96]\u001B[A\n",
      "Epoch 1:  38%|███▊      | 8/21 [00:15<00:21,  1.67s/it, loss=1.73]\u001B[A\n",
      "Epoch 1:  43%|████▎     | 9/21 [00:15<00:20,  1.68s/it, loss=1.73]\u001B[A\n",
      "Epoch 1:  43%|████▎     | 9/21 [00:16<00:20,  1.68s/it, loss=1.73]\u001B[A\n",
      "Epoch 1:  43%|████▎     | 9/21 [00:16<00:20,  1.68s/it, loss=1.53]\u001B[A\n",
      "Epoch 1:  48%|████▊     | 10/21 [00:16<00:18,  1.67s/it, loss=1.53]\u001B[A\n",
      "Epoch 1:  48%|████▊     | 10/21 [00:18<00:18,  1.67s/it, loss=1.53]\u001B[A\n",
      "Epoch 1:  48%|████▊     | 10/21 [00:18<00:18,  1.67s/it, loss=1.91]\u001B[A\n",
      "Epoch 1:  52%|█████▏    | 11/21 [00:18<00:16,  1.67s/it, loss=1.91]\u001B[A\n",
      "Epoch 1:  52%|█████▏    | 11/21 [00:20<00:16,  1.67s/it, loss=1.91]\u001B[A\n",
      "Epoch 1:  52%|█████▏    | 11/21 [00:20<00:16,  1.67s/it, loss=1.54]\u001B[A\n",
      "Epoch 1:  57%|█████▋    | 12/21 [00:20<00:14,  1.67s/it, loss=1.54]\u001B[A\n",
      "Epoch 1:  57%|█████▋    | 12/21 [00:21<00:14,  1.67s/it, loss=1.54]\u001B[A\n",
      "Epoch 1:  57%|█████▋    | 12/21 [00:21<00:14,  1.67s/it, loss=1.78]\u001B[A\n",
      "Epoch 1:  62%|██████▏   | 13/21 [00:21<00:13,  1.67s/it, loss=1.78]\u001B[A\n",
      "Epoch 1:  62%|██████▏   | 13/21 [00:23<00:13,  1.67s/it, loss=1.78]\u001B[A\n",
      "Epoch 1:  62%|██████▏   | 13/21 [00:23<00:13,  1.67s/it, loss=0.901]\u001B[A\n",
      "Epoch 1:  67%|██████▋   | 14/21 [00:23<00:11,  1.66s/it, loss=0.901]\u001B[A\n",
      "Epoch 1:  67%|██████▋   | 14/21 [00:25<00:11,  1.66s/it, loss=0.901]\u001B[A\n",
      "Epoch 1:  67%|██████▋   | 14/21 [00:25<00:11,  1.66s/it, loss=1.93] \u001B[A\n",
      "Epoch 1:  71%|███████▏  | 15/21 [00:25<00:09,  1.65s/it, loss=1.93]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 15/21 [00:26<00:09,  1.65s/it, loss=1.93]\u001B[A\n",
      "Epoch 1:  71%|███████▏  | 15/21 [00:26<00:09,  1.65s/it, loss=0.353]\u001B[A\n",
      "Epoch 1:  76%|███████▌  | 16/21 [00:26<00:08,  1.65s/it, loss=0.353]\u001B[A\n",
      "Epoch 1:  76%|███████▌  | 16/21 [00:28<00:08,  1.65s/it, loss=0.353]\u001B[A\n",
      "Epoch 1:  76%|███████▌  | 16/21 [00:28<00:08,  1.65s/it, loss=0.47] \u001B[A\n",
      "Epoch 1:  81%|████████  | 17/21 [00:28<00:06,  1.65s/it, loss=0.47]\u001B[A\n",
      "Epoch 1:  81%|████████  | 17/21 [00:29<00:06,  1.65s/it, loss=0.47]\u001B[A\n",
      "Epoch 1:  81%|████████  | 17/21 [00:29<00:06,  1.65s/it, loss=0.549]\u001B[A\n",
      "Epoch 1:  86%|████████▌ | 18/21 [00:29<00:04,  1.65s/it, loss=0.549]\u001B[A\n",
      "Epoch 1:  86%|████████▌ | 18/21 [00:31<00:04,  1.65s/it, loss=0.549]\u001B[A\n",
      "Epoch 1:  86%|████████▌ | 18/21 [00:31<00:04,  1.65s/it, loss=1.76] \u001B[A\n",
      "Epoch 1:  90%|█████████ | 19/21 [00:31<00:03,  1.66s/it, loss=1.76]\u001B[A\n",
      "Epoch 1:  90%|█████████ | 19/21 [00:33<00:03,  1.66s/it, loss=1.76]\u001B[A\n",
      "Epoch 1:  90%|█████████ | 19/21 [00:33<00:03,  1.66s/it, loss=2.16]\u001B[A\n",
      "Epoch 1:  95%|█████████▌| 20/21 [00:33<00:01,  1.67s/it, loss=2.16]\u001B[A\n",
      "Epoch 1:  95%|█████████▌| 20/21 [00:35<00:01,  1.67s/it, loss=2.16]\u001B[A\n",
      "Epoch 1:  95%|█████████▌| 20/21 [00:35<00:01,  1.67s/it, loss=1.45]\u001B[A\n",
      "Epoch 1: 100%|██████████| 21/21 [00:35<00:00,  1.68s/it, loss=1.45]\u001B[A\n",
      "\n",
      "  0%|          | 0/21 [00:00<?, ?it/s]\u001B[A\n",
      "Epoch 2:   0%|          | 0/21 [00:01<?, ?it/s]\u001B[A\n",
      "Epoch 2:   0%|          | 0/21 [00:01<?, ?it/s, loss=0.772]\u001B[A\n",
      "Epoch 2:   5%|▍         | 1/21 [00:01<00:38,  1.95s/it, loss=0.772]\u001B[A\n",
      "Epoch 2:   5%|▍         | 1/21 [00:03<00:38,  1.95s/it, loss=0.772]\u001B[A\n",
      "Epoch 2:   5%|▍         | 1/21 [00:03<00:38,  1.95s/it, loss=1.74] \u001B[A\n",
      "Epoch 2:  10%|▉         | 2/21 [00:03<00:34,  1.83s/it, loss=1.74]\u001B[A\n",
      "Epoch 2:  10%|▉         | 2/21 [00:05<00:34,  1.83s/it, loss=1.74]\u001B[A\n",
      "Epoch 2:  10%|▉         | 2/21 [00:05<00:34,  1.83s/it, loss=1.42]\u001B[A\n",
      "Epoch 2:  14%|█▍        | 3/21 [00:05<00:32,  1.81s/it, loss=1.42]\u001B[A\n",
      "Epoch 2:  14%|█▍        | 3/21 [00:07<00:32,  1.81s/it, loss=1.42]\u001B[A\n",
      "Epoch 2:  14%|█▍        | 3/21 [00:07<00:32,  1.81s/it, loss=1.68]\u001B[A\n",
      "Epoch 2:  19%|█▉        | 4/21 [00:07<00:30,  1.78s/it, loss=1.68]\u001B[A\n",
      "Epoch 2:  19%|█▉        | 4/21 [00:09<00:30,  1.78s/it, loss=1.68]\u001B[A\n",
      "Epoch 2:  19%|█▉        | 4/21 [00:09<00:30,  1.78s/it, loss=0.53]\u001B[A\n",
      "Epoch 2:  24%|██▍       | 5/21 [00:09<00:28,  1.79s/it, loss=0.53]\u001B[A\n",
      "Epoch 2:  24%|██▍       | 5/21 [00:10<00:28,  1.79s/it, loss=0.53]\u001B[A\n",
      "Epoch 2:  24%|██▍       | 5/21 [00:10<00:28,  1.79s/it, loss=1.66]\u001B[A\n",
      "Epoch 2:  29%|██▊       | 6/21 [00:10<00:26,  1.76s/it, loss=1.66]\u001B[A\n",
      "Epoch 2:  29%|██▊       | 6/21 [00:12<00:26,  1.76s/it, loss=1.66]\u001B[A\n",
      "Epoch 2:  29%|██▊       | 6/21 [00:12<00:26,  1.76s/it, loss=1.72]\u001B[A\n",
      "Epoch 2:  33%|███▎      | 7/21 [00:12<00:24,  1.76s/it, loss=1.72]\u001B[A\n",
      "Epoch 2:  33%|███▎      | 7/21 [00:14<00:24,  1.76s/it, loss=1.72]\u001B[A\n",
      "Epoch 2:  33%|███▎      | 7/21 [00:14<00:24,  1.76s/it, loss=1.43]\u001B[A\n",
      "Epoch 2:  38%|███▊      | 8/21 [00:14<00:22,  1.72s/it, loss=1.43]\u001B[A\n",
      "Epoch 2:  38%|███▊      | 8/21 [00:15<00:22,  1.72s/it, loss=1.43]\u001B[A\n",
      "Epoch 2:  38%|███▊      | 8/21 [00:15<00:22,  1.72s/it, loss=2.59]\u001B[A\n",
      "Epoch 2:  43%|████▎     | 9/21 [00:15<00:20,  1.70s/it, loss=2.59]\u001B[A\n",
      "Epoch 2:  43%|████▎     | 9/21 [00:17<00:20,  1.70s/it, loss=2.59]\u001B[A\n",
      "Epoch 2:  43%|████▎     | 9/21 [00:17<00:20,  1.70s/it, loss=0.613]\u001B[A\n",
      "Epoch 2:  48%|████▊     | 10/21 [00:17<00:18,  1.69s/it, loss=0.613]\u001B[A\n",
      "Epoch 2:  48%|████▊     | 10/21 [00:19<00:18,  1.69s/it, loss=0.613]\u001B[A\n",
      "Epoch 2:  48%|████▊     | 10/21 [00:19<00:18,  1.69s/it, loss=2]    \u001B[A\n",
      "Epoch 2:  52%|█████▏    | 11/21 [00:19<00:16,  1.68s/it, loss=2]\u001B[A\n",
      "Epoch 2:  52%|█████▏    | 11/21 [00:20<00:16,  1.68s/it, loss=2]\u001B[A\n",
      "Epoch 2:  52%|█████▏    | 11/21 [00:20<00:16,  1.68s/it, loss=0.324]\u001B[A\n",
      "Epoch 2:  57%|█████▋    | 12/21 [00:20<00:15,  1.67s/it, loss=0.324]\u001B[A\n",
      "Epoch 2:  57%|█████▋    | 12/21 [00:22<00:15,  1.67s/it, loss=0.324]\u001B[A\n",
      "Epoch 2:  57%|█████▋    | 12/21 [00:22<00:15,  1.67s/it, loss=0.809]\u001B[A\n",
      "Epoch 2:  62%|██████▏   | 13/21 [00:22<00:13,  1.67s/it, loss=0.809]\u001B[A\n",
      "Epoch 2:  62%|██████▏   | 13/21 [00:24<00:13,  1.67s/it, loss=0.809]\u001B[A\n",
      "Epoch 2:  62%|██████▏   | 13/21 [00:24<00:13,  1.67s/it, loss=0.555]\u001B[A\n",
      "Epoch 2:  67%|██████▋   | 14/21 [00:24<00:11,  1.67s/it, loss=0.555]\u001B[A\n",
      "Epoch 2:  67%|██████▋   | 14/21 [00:25<00:11,  1.67s/it, loss=0.555]\u001B[A\n",
      "Epoch 2:  67%|██████▋   | 14/21 [00:25<00:11,  1.67s/it, loss=2.73] \u001B[A\n",
      "Epoch 2:  71%|███████▏  | 15/21 [00:25<00:09,  1.66s/it, loss=2.73]\u001B[A\n",
      "Epoch 2:  71%|███████▏  | 15/21 [00:27<00:09,  1.66s/it, loss=2.73]\u001B[A\n",
      "Epoch 2:  71%|███████▏  | 15/21 [00:27<00:09,  1.66s/it, loss=2.25]\u001B[A\n",
      "Epoch 2:  76%|███████▌  | 16/21 [00:27<00:08,  1.65s/it, loss=2.25]\u001B[A\n",
      "Epoch 2:  76%|███████▌  | 16/21 [00:28<00:08,  1.65s/it, loss=2.25]\u001B[A\n",
      "Epoch 2:  76%|███████▌  | 16/21 [00:28<00:08,  1.65s/it, loss=1.49]\u001B[A\n",
      "Epoch 2:  81%|████████  | 17/21 [00:28<00:06,  1.65s/it, loss=1.49]\u001B[A\n",
      "Epoch 2:  81%|████████  | 17/21 [00:30<00:06,  1.65s/it, loss=1.49]\u001B[A\n",
      "Epoch 2:  81%|████████  | 17/21 [00:30<00:06,  1.65s/it, loss=2.64]\u001B[A\n",
      "Epoch 2:  86%|████████▌ | 18/21 [00:30<00:04,  1.66s/it, loss=2.64]\u001B[A\n",
      "Epoch 2:  86%|████████▌ | 18/21 [00:32<00:04,  1.66s/it, loss=2.64]\u001B[A\n",
      "Epoch 2:  86%|████████▌ | 18/21 [00:32<00:04,  1.66s/it, loss=0.401]\u001B[A\n",
      "Epoch 2:  90%|█████████ | 19/21 [00:32<00:03,  1.65s/it, loss=0.401]\u001B[A\n",
      "Epoch 2:  90%|█████████ | 19/21 [00:33<00:03,  1.65s/it, loss=0.401]\u001B[A\n",
      "Epoch 2:  90%|█████████ | 19/21 [00:33<00:03,  1.65s/it, loss=1.32] \u001B[A\n",
      "Epoch 2:  95%|█████████▌| 20/21 [00:33<00:01,  1.65s/it, loss=1.32]\u001B[A\n",
      "Epoch 2:  95%|█████████▌| 20/21 [00:35<00:01,  1.65s/it, loss=1.32]\u001B[A\n",
      "Epoch 2:  95%|█████████▌| 20/21 [00:35<00:01,  1.65s/it, loss=0.52]\u001B[A\n",
      "Epoch 2: 100%|██████████| 21/21 [00:35<00:00,  1.69s/it, loss=0.52]\u001B[A\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm  # for displaying progress bar\n",
    "\n",
    "# Load the FAQ data from the JSON file\n",
    "with open('BU_MET_FAQs.json', 'r') as file:\n",
    "    faq_data = json.load(file)\n",
    "\n",
    "class FAQDataset(Dataset):\n",
    "    def __init__(self, tokenizer, questions):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.examples = []\n",
    "\n",
    "        for item in questions:\n",
    "            answer_text = item['answer']\n",
    "            for url in item['urls']:\n",
    "                answer_text += f\" {url['text']} ({url['url']}) - {url['content']}\"\n",
    "            \n",
    "            prompt = f\"{item['question']} [Module: {item['module']}]\"\n",
    "            self.examples.append(tokenizer(prompt, answer_text, truncation=True, padding='max_length', max_length=512))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {key: torch.tensor(val) for key, val in self.examples[i].items()}\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Prepare the data for the GPT model\n",
    "questions = [{'question': data['question'], 'module': data['module'], 'answer': data['answer'], 'urls': data['urls']} for data in faq_data]\n",
    "dataset = FAQDataset(tokenizer, questions)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.train()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['input_ids'])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./trained_gpt_faq_model')\n",
    "\n",
    "# Function to generate answers\n",
    "def ask_question(question, module):\n",
    "    model.eval()\n",
    "    prompt = f\"{question} [Module: {module}]\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=200)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T00:48:22.014320Z",
     "start_time": "2024-06-12T00:46:33.277113Z"
    }
   },
   "id": "110050e1534d0dff"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I drop classes from my schedule? [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [Module: Registration] [\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "print(ask_question(\"How can I drop classes from my schedule?\", \"Registration\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-12T00:48:38.369784Z",
     "start_time": "2024-06-12T00:48:31.742452Z"
    }
   },
   "id": "8607354cda861a"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f872eb70798a4bd985d23761c3a0e669"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88fceff9e6ca49f5b5dea39d251bd993"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d48757bd6964ab1983d0aed26d0ac1d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e32819c8e60c468eba1eead1336d4d76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/105 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "990b32b39f5d4ed59f7e40bd373368fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 假设tokenizer和模型已经准备好\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-large')\n",
    "model.train()\n",
    "\n",
    "# 加载数据集\n",
    "questions = [{'question': data['question'], 'module': data['module'], 'answer': data['answer'], 'urls': data['urls']} for data in faq_data]\n",
    "dataset = FAQDataset(tokenizer, questions)  # 确保已正确实现FAQDataset类\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# 优化器与学习率调度器\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=5 * len(data_loader))\n",
    "\n",
    "progress_bar = tqdm(range(5 * len(data_loader)))\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(5):\n",
    "    for batch in data_loader:\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['input_ids'])\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "model.eval()  # 切换到评估模式"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T21:41:02.571983Z",
     "start_time": "2024-06-11T21:40:09.226231Z"
    }
   },
   "id": "5c9f2d399ccfdd2d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
